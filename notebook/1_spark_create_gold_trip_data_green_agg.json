{
	"name": "1_spark_create_gold_trip_data_green_agg",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "trippoolX",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "03dcf1ad-74b1-4aac-a968-1a83b87c4ca4"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/1216dbb4-2bbc-44f4-9224-1147ecde1e00/resourceGroups/synapse-course/providers/Microsoft.Synapse/workspaces/synapse-course-wss/bigDataPools/trippoolX",
				"name": "trippoolX",
				"type": "Spark",
				"endpoint": "https://synapse-course-wss.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/trippoolX",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 5
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Trip Data Aggregation \n",
					"### Group By Columns\n",
					"1. year\n",
					"2. Month\n",
					"3. Pickup Location ID\n",
					"4. Drop Off Location ID\n",
					"\n",
					"### Aggregated Columns\n",
					"1. Total Trip Count\n",
					"2. Total Fare Amount\n",
					"\n",
					"### Purpose of the notebook\n",
					"\n",
					"Demonstrate the integration between Spark Pool and Serverless SQL Pool\n",
					"\n",
					"1. Create the aggregated table in Spark Pool\n",
					"2. Access the data from Serverless SQL Pool "
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#Set the folder paths so that it can be used later. \n",
					"bronze_folder_path = 'abfss://nyc-taxi-data@synapsecoursedllll.dfs.core.windows.net/raw'\n",
					"silver_folder_path = 'abfss://nyc-taxi-data@synapsecoursedllll.dfs.core.windows.net/silver'\n",
					"gold_folder_path = 'abfss://nyc-taxi-data@synapsecoursedllll.dfs.core.windows.net/gold'\n",
					"/"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#Set the spark config to be able to get the partitioned columns year and month as strings rather than integers\n",
					"spark.conf.set(\"spark.sql.sources.partitionColumnTypeInference.enabled\", \"false\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"-- Create database to which we are going to write the data\n",
					"CREATE DATABASE IF NOT EXISTS nyc_taxi_ldw_spark\n",
					"LOCATION 'abfss://nyc-taxi-data@synapsecoursedllll.dfs.core.windows.net/gold';"
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Read the silver data to be processed. \n",
					"trip_data_green_df = spark.read.parquet(f\"{silver_folder_path}/trip_data_green\") "
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Perform the required aggregations\n",
					"# 1. Total trip count\n",
					"# 2. Total fare amount\n",
					"from pyspark.sql.functions import *\n",
					"\n",
					"trip_data_green_agg_df = trip_data_green_df \\\n",
					"                        .groupBy(\"year\", \"month\", \"pu_location_id\", \"do_location_id\") \\\n",
					"                        .agg(count(lit(1)).alias(\"total_trip_count\"),\n",
					"                        round(sum(\"fare_amount\"), 2).alias(\"total_fare_amount\"))"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Write the aggregated data to the gold table for consumption\n",
					"trip_data_green_agg_df.write.mode(\"overwrite\").partitionBy(\"year\", \"month\").format(\"parquet\").saveAsTable(\"nyc_taxi_ldw_spark.trip_data_green_agg\")"
				],
				"execution_count": 6
			}
		]
	}
}